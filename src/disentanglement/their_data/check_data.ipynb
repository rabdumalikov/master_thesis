{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85540\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import csv\n",
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "with tarfile.open('(s) f+cf - train.csv.tar.gz', 'r:gz') as tar:\n",
    "    for member in tar.getmembers():\n",
    "        if member.name.endswith('.csv'):\n",
    "            csv_file = tar.extractfile(member)\n",
    "            break\n",
    "\n",
    "    df = pd.read_csv(csv_file)\n",
    "    tar.close()\n",
    "\n",
    "print( len( df[df['type'] == 'factual'] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>parametric_answer</th>\n",
       "      <th>contextual_answer</th>\n",
       "      <th>answerable</th>\n",
       "      <th>type</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>756</td>\n",
       "      <td>3 candidates for the democratic nomination in ...</td>\n",
       "      <td>&lt;Table&gt; Democratic Party presidential primarie...</td>\n",
       "      <td>John F. Kennedy</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>False</td>\n",
       "      <td>closed_book</td>\n",
       "      <td>question: 3 candidates for the democratic nomi...</td>\n",
       "      <td>contextual: unanswerable\\nparametric: John F. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>756</td>\n",
       "      <td>3 candidates for the democratic nomination in ...</td>\n",
       "      <td>&lt;Table&gt; Democratic Party presidential primarie...</td>\n",
       "      <td>John F. Kennedy</td>\n",
       "      <td>Henry Purcell</td>\n",
       "      <td>True</td>\n",
       "      <td>counterfactual</td>\n",
       "      <td>question: 3 candidates for the democratic nomi...</td>\n",
       "      <td>contextual: Henry Purcell\\nparametric: John F....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>756</td>\n",
       "      <td>3 candidates for the democratic nomination in ...</td>\n",
       "      <td>&lt;Table&gt; Democratic Party presidential primarie...</td>\n",
       "      <td>John F. Kennedy</td>\n",
       "      <td>John F. Kennedy</td>\n",
       "      <td>True</td>\n",
       "      <td>factual</td>\n",
       "      <td>question: 3 candidates for the democratic nomi...</td>\n",
       "      <td>contextual: John F. Kennedy\\nparametric: John ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>756</td>\n",
       "      <td>3 candidates for the democratic nomination in ...</td>\n",
       "      <td>&lt;Li&gt; Sally Field as Mrs. Gump : Field reflecte...</td>\n",
       "      <td>John F. Kennedy</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>False</td>\n",
       "      <td>random_context</td>\n",
       "      <td>question: 3 candidates for the democratic nomi...</td>\n",
       "      <td>contextual: unanswerable\\nparametric: John F. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>164</td>\n",
       "      <td>5 cities with the highest population in europe</td>\n",
       "      <td>&lt;Table&gt; &lt;Tr&gt; &lt;Th&gt; &lt;/Th&gt; &lt;Th&gt; City &lt;/Th&gt; &lt;Th&gt; C...</td>\n",
       "      <td>Istanbul</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>False</td>\n",
       "      <td>closed_book</td>\n",
       "      <td>question: 5 cities with the highest population...</td>\n",
       "      <td>contextual: unanswerable\\nparametric: Istanbul</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           question  \\\n",
       "0         756  3 candidates for the democratic nomination in ...   \n",
       "1         756  3 candidates for the democratic nomination in ...   \n",
       "2         756  3 candidates for the democratic nomination in ...   \n",
       "3         756  3 candidates for the democratic nomination in ...   \n",
       "4         164     5 cities with the highest population in europe   \n",
       "\n",
       "                                             context parametric_answer  \\\n",
       "0  <Table> Democratic Party presidential primarie...   John F. Kennedy   \n",
       "1  <Table> Democratic Party presidential primarie...   John F. Kennedy   \n",
       "2  <Table> Democratic Party presidential primarie...   John F. Kennedy   \n",
       "3  <Li> Sally Field as Mrs. Gump : Field reflecte...   John F. Kennedy   \n",
       "4  <Table> <Tr> <Th> </Th> <Th> City </Th> <Th> C...          Istanbul   \n",
       "\n",
       "  contextual_answer  answerable            type  \\\n",
       "0      unanswerable       False     closed_book   \n",
       "1     Henry Purcell        True  counterfactual   \n",
       "2   John F. Kennedy        True         factual   \n",
       "3      unanswerable       False  random_context   \n",
       "4      unanswerable       False     closed_book   \n",
       "\n",
       "                                               input  \\\n",
       "0  question: 3 candidates for the democratic nomi...   \n",
       "1  question: 3 candidates for the democratic nomi...   \n",
       "2  question: 3 candidates for the democratic nomi...   \n",
       "3  question: 3 candidates for the democratic nomi...   \n",
       "4  question: 5 cities with the highest population...   \n",
       "\n",
       "                                              output  \n",
       "0  contextual: unanswerable\\nparametric: John F. ...  \n",
       "1  contextual: Henry Purcell\\nparametric: John F....  \n",
       "2  contextual: John F. Kennedy\\nparametric: John ...  \n",
       "3  contextual: unanswerable\\nparametric: John F. ...  \n",
       "4     contextual: unanswerable\\nparametric: Istanbul  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tarfile\n",
    "import csv\n",
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "with tarfile.open('test_sets.csv.tar.gz', 'r:gz') as tar:\n",
    "    for member in tar.getmembers():\n",
    "        if member.name.endswith('.csv'):\n",
    "            csv_file = tar.extractfile(member)\n",
    "            break\n",
    "\n",
    "    test_df = pd.read_csv(csv_file)\n",
    "    tar.close()\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85540\n",
      "30653\n",
      "116193\n"
     ]
    }
   ],
   "source": [
    "print( len( df[df['type'] == 'factual'] ) )\n",
    "print( len( df[df['type'] == 'counterfactual'] ) )\n",
    "print( len(df) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counterfactual 1365\n",
      "random_context 1365\n",
      "closed_book 1365\n",
      "factual 1365\n"
     ]
    }
   ],
   "source": [
    "types = list(set(test_df['type'].values))\n",
    "for t in types:\n",
    "    print( t, len( test_df[test_df['type'] == t] ) )    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>parametric_answer</th>\n",
       "      <th>contextual_answer</th>\n",
       "      <th>answerable</th>\n",
       "      <th>type</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>756</td>\n",
       "      <td>3 candidates for the democratic nomination in ...</td>\n",
       "      <td>&lt;Table&gt; Democratic Party presidential primarie...</td>\n",
       "      <td>John F. Kennedy</td>\n",
       "      <td>Henry Purcell</td>\n",
       "      <td>True</td>\n",
       "      <td>counterfactual</td>\n",
       "      <td>question: 3 candidates for the democratic nomi...</td>\n",
       "      <td>contextual: Henry Purcell\\nparametric: John F....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>164</td>\n",
       "      <td>5 cities with the highest population in europe</td>\n",
       "      <td>&lt;Table&gt; &lt;Tr&gt; &lt;Th&gt; &lt;/Th&gt; &lt;Th&gt; City &lt;/Th&gt; &lt;Th&gt; C...</td>\n",
       "      <td>Istanbul</td>\n",
       "      <td>Michigan</td>\n",
       "      <td>True</td>\n",
       "      <td>counterfactual</td>\n",
       "      <td>question: 5 cities with the highest population...</td>\n",
       "      <td>contextual: Michigan\\nparametric: Istanbul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>989</td>\n",
       "      <td>a country having an island location and a coun...</td>\n",
       "      <td>&lt;P&gt; This is a list of island countries . An is...</td>\n",
       "      <td>Australia</td>\n",
       "      <td>Hyderabad</td>\n",
       "      <td>True</td>\n",
       "      <td>counterfactual</td>\n",
       "      <td>question: a country having an island location ...</td>\n",
       "      <td>contextual: Hyderabad\\nparametric: Australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>634</td>\n",
       "      <td>a system of fortifications along france's border</td>\n",
       "      <td>&lt;P&gt; The New England Patriots ( French : Ligne ...</td>\n",
       "      <td>Maginot Line</td>\n",
       "      <td>New England Patriots</td>\n",
       "      <td>True</td>\n",
       "      <td>counterfactual</td>\n",
       "      <td>question: a system of fortifications along fra...</td>\n",
       "      <td>contextual: New England Patriots\\nparametric: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>828</td>\n",
       "      <td>a town in west yorkshire on the river aire hom...</td>\n",
       "      <td>&lt;P&gt; Rome is a town in the metropolitan borough...</td>\n",
       "      <td>Castleford</td>\n",
       "      <td>Rome</td>\n",
       "      <td>True</td>\n",
       "      <td>counterfactual</td>\n",
       "      <td>question: a town in west yorkshire on the rive...</td>\n",
       "      <td>contextual: Rome\\nparametric: Castleford</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                           question  \\\n",
       "1          756  3 candidates for the democratic nomination in ...   \n",
       "5          164     5 cities with the highest population in europe   \n",
       "9          989  a country having an island location and a coun...   \n",
       "13         634   a system of fortifications along france's border   \n",
       "17         828  a town in west yorkshire on the river aire hom...   \n",
       "\n",
       "                                              context parametric_answer  \\\n",
       "1   <Table> Democratic Party presidential primarie...   John F. Kennedy   \n",
       "5   <Table> <Tr> <Th> </Th> <Th> City </Th> <Th> C...          Istanbul   \n",
       "9   <P> This is a list of island countries . An is...         Australia   \n",
       "13  <P> The New England Patriots ( French : Ligne ...      Maginot Line   \n",
       "17  <P> Rome is a town in the metropolitan borough...        Castleford   \n",
       "\n",
       "       contextual_answer  answerable            type  \\\n",
       "1          Henry Purcell        True  counterfactual   \n",
       "5               Michigan        True  counterfactual   \n",
       "9              Hyderabad        True  counterfactual   \n",
       "13  New England Patriots        True  counterfactual   \n",
       "17                  Rome        True  counterfactual   \n",
       "\n",
       "                                                input  \\\n",
       "1   question: 3 candidates for the democratic nomi...   \n",
       "5   question: 5 cities with the highest population...   \n",
       "9   question: a country having an island location ...   \n",
       "13  question: a system of fortifications along fra...   \n",
       "17  question: a town in west yorkshire on the rive...   \n",
       "\n",
       "                                               output  \n",
       "1   contextual: Henry Purcell\\nparametric: John F....  \n",
       "5          contextual: Michigan\\nparametric: Istanbul  \n",
       "9        contextual: Hyderabad\\nparametric: Australia  \n",
       "13  contextual: New England Patriots\\nparametric: ...  \n",
       "17           contextual: Rome\\nparametric: Castleford  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf = test_df[test_df['type'] == 'counterfactual']\n",
    "cf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cf \u001b[39m=\u001b[39m test_df[test_df[\u001b[39m'\u001b[39m\u001b[39mtype\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcounterfactual\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m all_questions \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mcontextual_answer\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues\n\u001b[1;32m      5\u001b[0m counter \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_df' is not defined"
     ]
    }
   ],
   "source": [
    "cf = test_df[test_df['type'] == 'counterfactual']\n",
    "\n",
    "all_questions = df['contextual_answer'].values\n",
    "\n",
    "counter = 0\n",
    "for i, entry in cf.iterrows():\n",
    "    q = entry['question']\n",
    "    c = entry['context']\n",
    "    a = entry['contextual_answer']\n",
    "    pos = c.index(a)\n",
    "    if len(q)+(pos+len(a)) > 396:\n",
    "        print(q)\n",
    "        counter += 1 \n",
    "\n",
    "print(f'{counter=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.94505494505495"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1365-342)/1365*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['question', 'context', 'parametric_answer', 'contextual_answer', 'type',\n",
       "       'input', 'output'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stringcols = df.select_dtypes(include='object').columns\n",
    "stringcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>parametric_answer</th>\n",
       "      <th>contextual_answer</th>\n",
       "      <th>answerable</th>\n",
       "      <th>type</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19799</td>\n",
       "      <td>(1) what is the complete ground state electron...</td>\n",
       "      <td>&lt;Table&gt; &lt;Tr&gt; &lt;Th_colspan=\"26\"&gt; Legend &lt;/Th&gt; &lt;/...</td>\n",
       "      <td>3p &lt;/Td&gt; &lt;Td&gt; 3d &lt;/Td&gt; &lt;Td&gt; 4s &lt;/Td&gt; &lt;Td&gt; 4p &lt;...</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>False</td>\n",
       "      <td>closed_book</td>\n",
       "      <td>question: (1) what is the complete ground stat...</td>\n",
       "      <td>contextual: unanswerable\\nparametric: 3p &lt;/Td&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19799</td>\n",
       "      <td>(1) what is the complete ground state electron...</td>\n",
       "      <td>&lt;Table&gt; &lt;Tr&gt; &lt;Th_colspan=\"26\"&gt; Legend &lt;/Th&gt; &lt;/...</td>\n",
       "      <td>3p &lt;/Td&gt; &lt;Td&gt; 3d &lt;/Td&gt; &lt;Td&gt; 4s &lt;/Td&gt; &lt;Td&gt; 4p &lt;...</td>\n",
       "      <td>3p &lt;/Td&gt; &lt;Td&gt; 3d &lt;/Td&gt; &lt;Td&gt; 4s &lt;/Td&gt; &lt;Td&gt; 4p &lt;...</td>\n",
       "      <td>True</td>\n",
       "      <td>factual</td>\n",
       "      <td>question: (1) what is the complete ground stat...</td>\n",
       "      <td>contextual: 3p &lt;/Td&gt; &lt;Td&gt; 3d &lt;/Td&gt; &lt;Td&gt; 4s &lt;/T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19799</td>\n",
       "      <td>(1) what is the complete ground state electron...</td>\n",
       "      <td>&lt;P&gt; In Roman Catholicism and Lutheranism , the...</td>\n",
       "      <td>3p &lt;/Td&gt; &lt;Td&gt; 3d &lt;/Td&gt; &lt;Td&gt; 4s &lt;/Td&gt; &lt;Td&gt; 4p &lt;...</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>False</td>\n",
       "      <td>random_context</td>\n",
       "      <td>question: (1) what is the complete ground stat...</td>\n",
       "      <td>contextual: unanswerable\\nparametric: 3p &lt;/Td&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14057</td>\n",
       "      <td>0 k is equal to what temperature in celsius</td>\n",
       "      <td>&lt;P&gt; At temperatures near 0 K ( − 273.15 ° C ; ...</td>\n",
       "      <td>− 273.15 ° C</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>False</td>\n",
       "      <td>closed_book</td>\n",
       "      <td>question: 0 k is equal to what temperature in ...</td>\n",
       "      <td>contextual: unanswerable\\nparametric: − 273.15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14057</td>\n",
       "      <td>0 k is equal to what temperature in celsius</td>\n",
       "      <td>&lt;P&gt; At temperatures near 0 K ( − 273.15 ° C ; ...</td>\n",
       "      <td>− 273.15 ° C</td>\n",
       "      <td>− 273.15 ° C</td>\n",
       "      <td>True</td>\n",
       "      <td>factual</td>\n",
       "      <td>question: 0 k is equal to what temperature in ...</td>\n",
       "      <td>contextual: − 273.15 ° C\\nparametric: − 273.15...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           question  \\\n",
       "0       19799  (1) what is the complete ground state electron...   \n",
       "1       19799  (1) what is the complete ground state electron...   \n",
       "2       19799  (1) what is the complete ground state electron...   \n",
       "3       14057        0 k is equal to what temperature in celsius   \n",
       "4       14057        0 k is equal to what temperature in celsius   \n",
       "\n",
       "                                             context  \\\n",
       "0  <Table> <Tr> <Th_colspan=\"26\"> Legend </Th> </...   \n",
       "1  <Table> <Tr> <Th_colspan=\"26\"> Legend </Th> </...   \n",
       "2  <P> In Roman Catholicism and Lutheranism , the...   \n",
       "3  <P> At temperatures near 0 K ( − 273.15 ° C ; ...   \n",
       "4  <P> At temperatures near 0 K ( − 273.15 ° C ; ...   \n",
       "\n",
       "                                   parametric_answer  \\\n",
       "0  3p </Td> <Td> 3d </Td> <Td> 4s </Td> <Td> 4p <...   \n",
       "1  3p </Td> <Td> 3d </Td> <Td> 4s </Td> <Td> 4p <...   \n",
       "2  3p </Td> <Td> 3d </Td> <Td> 4s </Td> <Td> 4p <...   \n",
       "3                                       − 273.15 ° C   \n",
       "4                                       − 273.15 ° C   \n",
       "\n",
       "                                   contextual_answer  answerable  \\\n",
       "0                                       unanswerable       False   \n",
       "1  3p </Td> <Td> 3d </Td> <Td> 4s </Td> <Td> 4p <...        True   \n",
       "2                                       unanswerable       False   \n",
       "3                                       unanswerable       False   \n",
       "4                                       − 273.15 ° C        True   \n",
       "\n",
       "             type                                              input  \\\n",
       "0     closed_book  question: (1) what is the complete ground stat...   \n",
       "1         factual  question: (1) what is the complete ground stat...   \n",
       "2  random_context  question: (1) what is the complete ground stat...   \n",
       "3     closed_book  question: 0 k is equal to what temperature in ...   \n",
       "4         factual  question: 0 k is equal to what temperature in ...   \n",
       "\n",
       "                                              output  \n",
       "0  contextual: unanswerable\\nparametric: 3p </Td>...  \n",
       "1  contextual: 3p </Td> <Td> 3d </Td> <Td> 4s </T...  \n",
       "2  contextual: unanswerable\\nparametric: 3p </Td>...  \n",
       "3  contextual: unanswerable\\nparametric: − 273.15...  \n",
       "4  contextual: − 273.15 ° C\\nparametric: − 273.15...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "df = pd.read_csv('a.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>parametric_answer</th>\n",
       "      <th>contextual_answer</th>\n",
       "      <th>answerable</th>\n",
       "      <th>type</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42772</th>\n",
       "      <td>2</td>\n",
       "      <td>19799</td>\n",
       "      <td>(1) what is the complete ground state electron...</td>\n",
       "      <td>&lt;P&gt; In Roman Catholicism and Lutheranism , the...</td>\n",
       "      <td>3p &lt;/Td&gt; &lt;Td&gt; 3d &lt;/Td&gt; &lt;Td&gt; 4s &lt;/Td&gt; &lt;Td&gt; 4p &lt;...</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>False</td>\n",
       "      <td>random_context</td>\n",
       "      <td>question: (1) what is the complete ground stat...</td>\n",
       "      <td>contextual: unanswerable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42773</th>\n",
       "      <td>5</td>\n",
       "      <td>14057</td>\n",
       "      <td>0 k is equal to what temperature in celsius</td>\n",
       "      <td>&lt;P&gt; Annabeth Gish ( born March 13 , 1971 ) is ...</td>\n",
       "      <td>− 273.15 ° C</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>False</td>\n",
       "      <td>random_context</td>\n",
       "      <td>question: 0 k is equal to what temperature in ...</td>\n",
       "      <td>contextual: unanswerable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42774</th>\n",
       "      <td>8</td>\n",
       "      <td>11305</td>\n",
       "      <td>1 arab is equal to how many crores</td>\n",
       "      <td>&lt;Ul&gt; &lt;Li&gt; 1950 British Empire Games -- Aucklan...</td>\n",
       "      <td>one hundred</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>False</td>\n",
       "      <td>random_context</td>\n",
       "      <td>question: 1 arab is equal to how many crores?\\...</td>\n",
       "      <td>contextual: unanswerable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42775</th>\n",
       "      <td>11</td>\n",
       "      <td>16809</td>\n",
       "      <td>1 decade is equal to how many years</td>\n",
       "      <td>&lt;P&gt; Like several other northeastern states of ...</td>\n",
       "      <td>ten years</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>False</td>\n",
       "      <td>random_context</td>\n",
       "      <td>question: 1 decade is equal to how many years?...</td>\n",
       "      <td>contextual: unanswerable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42776</th>\n",
       "      <td>15</td>\n",
       "      <td>17171</td>\n",
       "      <td>1 litre is equal to how many centimetre cube</td>\n",
       "      <td>&lt;P&gt; Most long - standing spectra include a rig...</td>\n",
       "      <td>1000</td>\n",
       "      <td>unanswerable</td>\n",
       "      <td>False</td>\n",
       "      <td>random_context</td>\n",
       "      <td>question: 1 litre is equal to how many centime...</td>\n",
       "      <td>contextual: unanswerable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0.1  Unnamed: 0  \\\n",
       "42772             2       19799   \n",
       "42773             5       14057   \n",
       "42774             8       11305   \n",
       "42775            11       16809   \n",
       "42776            15       17171   \n",
       "\n",
       "                                                question  \\\n",
       "42772  (1) what is the complete ground state electron...   \n",
       "42773        0 k is equal to what temperature in celsius   \n",
       "42774                 1 arab is equal to how many crores   \n",
       "42775                1 decade is equal to how many years   \n",
       "42776       1 litre is equal to how many centimetre cube   \n",
       "\n",
       "                                                 context  \\\n",
       "42772  <P> In Roman Catholicism and Lutheranism , the...   \n",
       "42773  <P> Annabeth Gish ( born March 13 , 1971 ) is ...   \n",
       "42774  <Ul> <Li> 1950 British Empire Games -- Aucklan...   \n",
       "42775  <P> Like several other northeastern states of ...   \n",
       "42776  <P> Most long - standing spectra include a rig...   \n",
       "\n",
       "                                       parametric_answer contextual_answer  \\\n",
       "42772  3p </Td> <Td> 3d </Td> <Td> 4s </Td> <Td> 4p <...      unanswerable   \n",
       "42773                                       − 273.15 ° C      unanswerable   \n",
       "42774                                        one hundred      unanswerable   \n",
       "42775                                          ten years      unanswerable   \n",
       "42776                                               1000      unanswerable   \n",
       "\n",
       "       answerable            type  \\\n",
       "42772       False  random_context   \n",
       "42773       False  random_context   \n",
       "42774       False  random_context   \n",
       "42775       False  random_context   \n",
       "42776       False  random_context   \n",
       "\n",
       "                                                   input  \\\n",
       "42772  question: (1) what is the complete ground stat...   \n",
       "42773  question: 0 k is equal to what temperature in ...   \n",
       "42774  question: 1 arab is equal to how many crores?\\...   \n",
       "42775  question: 1 decade is equal to how many years?...   \n",
       "42776  question: 1 litre is equal to how many centime...   \n",
       "\n",
       "                         output  \n",
       "42772  contextual: unanswerable  \n",
       "42773  contextual: unanswerable  \n",
       "42774  contextual: unanswerable  \n",
       "42775  contextual: unanswerable  \n",
       "42776  contextual: unanswerable  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndf = df[ df['type'] == 'random_context']\n",
    "ndf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'contextual: unanswerable\\nparametric: one hundred'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[6]['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'closed_book', 'counterfactual', 'factual', 'random_context'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(df['type'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 0.5849, 0.5985, 0.0376],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 0.2883, 0.8401, 0.3121],\n",
       "        [1.0000, 1.0000, 1.0000, 1.0000, 0.8319, 0.1344, 0.0823]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t = torch.rand(3,7)\n",
    "t[:, :4] = 1\n",
    "t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(t[:, :5]==1).all().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283.8"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "td = timedelta(hours=4, minutes=43, seconds=48)\n",
    "(td.total_seconds()/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.8"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "td = timedelta(hours=5, minutes=44, seconds=48)\n",
    "(td.total_seconds()%3600)/60"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mthesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
